{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & NLP\n",
    "\n",
    "# Data Collection Notebook\n",
    "\n",
    "In this notebook, we use `Pushshift Reddit API` in order to scrape reddit submissions from `r/LifeProTips` and `r/YouShouldKnow`.\n",
    "\n",
    "The two chosen subreddits share similar contents in giving random bits of useful information to redditors. Firstly, in order to keep the data balanced, we will obtain 1500 posts from each subreddit. Secondly in order to weed out 'weak' posts, we will only take posts above a certain score threshold. Finally, we will clean some of the data before exporting them for modelling <a href=\"./Modelling.ipynb\">here</a>.\n",
    "\n",
    "For the final evalution and summary notebook, please click <a href=\"./Summary.ipynb\">here</a>.\n",
    "\n",
    "### Scraping the for Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(sub,score):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    df_concat_list = []\n",
    "    params = {\n",
    "        'subreddit': sub,\n",
    "        'size': 100,\n",
    "        'before': None,\n",
    "        'is_video':'false', # skip video posts\n",
    "        'stickied':'false', # skip on stickied posts (i.e. rules, megathreads, etc)\n",
    "        'score': '>'+str(score) # only scrape posts above a certain score\n",
    "    }\n",
    "    print(sub+\" Scraping Start\")\n",
    "    for i in range(15):\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        df_new = pd.DataFrame(posts)\n",
    "        df_new = df_new[['subreddit','id','title','selftext','created_utc']]\n",
    "        df_concat_list.append(df_new) #load the final df with the extracted data\n",
    "        params['before'] = df_new['created_utc'][len(df_new)-1] # continue the 'before' parameter after last cycle timestamp\n",
    "        time.sleep(20) # add delay to avoid overloading the API\n",
    "        print(f'Iteration {i+1}')\n",
    "    df = pd.concat(df_concat_list)\n",
    "    df.to_csv(f'./dataset/{sub}.csv', index = False) # export to csv after complete scraping\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LifeProTips` has a relatively high user population of 19.7m users at the time of scraping. Therefore, we set the score higher since there are more users to upvote the posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifeProTips Scraping Start\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "scrape(\"LifeProTips\",4500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YouShouldKnow` has a significantly lower user base compared to `LifeProTips`. Therefore, we scale the score down in case we reach the bottom of the list and run out of things to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouShouldKnow Scraping Start\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "scrape(\"YouShouldKnow\",150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Both subreddits have their respective post tags of \"LPT\" for `LifeProTips` and \"YSK\" for `YouShouldKnow`. This would make the sorting in the modelling process later too easy for the model. Therefore, we will remove the tags from each of the post titles. Furthermore, we will also concatenate the `title` and `selftext` columns to form `text` which will contain all of our post content.\n",
    "\n",
    "_Edit: After modelling it was discovered that URLs are mistakenly picked up as key words by the modelling process. We will remove all URLs to avoid their unwanted data. Secondly the models also picked up 000 as a key feature. We will convert it to 'k' in order to avoid splitting up potentially significant numbers._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(sub):\n",
    "    df = pd.read_csv(f'./dataset/{sub}.csv')\n",
    "    \n",
    "    df['selftext'].fillna(\".\",inplace=True) # fillna with '.' so we can concatenate 'title' and 'selftext'\n",
    "    df['text'] = df['title'] +' '+ df['selftext']\n",
    "    \n",
    "    # remove post tag at start of post\n",
    "    df['text'] = df['text'].str.replace(r\"^.?[LlPpTtYySsKk]{3}[^\\s]*\\s?[\\-\\:]?\\s?\",\"\")\n",
    "    \n",
    "    # remove [removed] tags\n",
    "    df['text'] = df['text'].str.replace(r'\\[removed\\]','')\n",
    "    \n",
    "    # remove any other LPT & YSK tags\n",
    "    df['text'] = df['text'].str.replace(r'ysk|YSK|lpt|LPT','')\n",
    "    \n",
    "    # remove url\n",
    "    df['text'] = df['text'].str.replace(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)','')    \n",
    "    \n",
    "    # replace \",000\" with k to avoid picking up \"000\" with cvec\n",
    "    df['text'] = df['text'].str.replace(r'\\,000','k')\n",
    "    \n",
    "    #export to same file\n",
    "    df.to_csv(f'./dataset/{sub}.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner(\"YouShouldKnow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner(\"LifeProTips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nullfind(df): # find missing values by columns\n",
    "    col_with_null = df.loc[:, df.isnull().any()].columns.tolist()\n",
    "    return df[col_with_null].isnull().sum()/len(df['id']) #% of missing values\n",
    "\n",
    "def duplicate_check(df): # check for duplicate entries by df shape\n",
    "    n_unique = df.id.nunique()\n",
    "    print(\"length of ID column: \"+ str(len(df.id)))\n",
    "    print(\"shape of DataFrame: \"+ str(df.shape))\n",
    "    print(f'no. of unique posts: {n_unique}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPT = pd.read_csv('./dataset/LifeProTips.csv')\n",
    "YSK = pd.read_csv('./dataset/YouShouldKnow.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ID column: 1472\n",
      "shape of DataFrame: (1472, 6)\n",
      "no. of unique posts: 1472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_check(LPT)\n",
    "nullfind(LPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of ID column: 1439\n",
      "shape of DataFrame: (1439, 6)\n",
      "no. of unique posts: 1439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_check(YSK)\n",
    "nullfind(YSK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we obverved above, the rows did not add up to 3,000. Missing data or image posts are dropped by the API and therefore we have a combined total of 2,921 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat both dataframe into 1 to export for feature engineering and modelling\n",
    "concat = pd.concat([LPT,YSK])\n",
    "concat.to_csv('./dataset/data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
